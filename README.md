
# MOBA-E2C:  Generating MOBA Game Commentaries  via Capturing Highlight Events  from the Meta-Data

The released MOBA Game Commentary Generation Dataset ``Dota2-Commentary``. Please refer to our paper  https://aclanthology.org/2022.findings-emnlp.333/.



# Abstract
MOBA (Multiplayer Online Battle Arena) games such as Dota2 are currently one of the most popular e-sports gaming genres. Following professional commentaries is a great way to understand and enjoy a MOBA game. However, massive game competitions lack commentaries because of the shortage of professional human commentators. As an alternative, employing machine commentators that can work at any time and place is a feasible solution. Considering the challenges in modeling MOBA games, we propose a data-driven MOBA commentary generation framework, MOBA-E2C, allowing a model to generate commentaries based on the game meta-data. Subsequently, to alleviate the burden of collecting supervised data, we propose a MOBA-FuseGPT generator to generate MOBA game commentaries by fusing the power of a rule-based generator and a generative GPT generator. Finally, in the experiments, we take a popular MOBA game Dota2 as our case and construct a Chinese Dota2 commentary generation dataset Dota2-Commentary. Experimental results demonstrate the superior performance of our approach. To the best of our knowledge, this work is the first Dota2 machine commentator and Dota2-Commentary is the first dataset.



# Dataset
![img.png](img.png)

Please check the folder ``data/`` for the detail.

- in ``data/general``, we provided the data without the machine generated commentaries.
- in ``data/machine``, we provided the data with the machine generated commentaries.

for each data partition you can use the `.json` file or the linearized `.src` and `.tgt`.

# Pre-trained Checkpoints

This project releases three trained/pre-trained model checkpoints. Note that all such tree checkpoints are based on an open-released [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese), which is different from the GPT2 originally reported by our paper because of the licence.


## Checkpoints
You can download such checkpoints from [Google Drive](https://drive.google.com/drive/folders/1uYoW24LGyEUws2aPLhR1ODpyZsBrwhuf?usp=share_link)  or  [Baidu Disk](https://pan.baidu.com/s/1ytSKhjxq0bKUvDa3Nc-BJA)  passwordï¼šuvkn.

There are three checkpoints:

- `model/MOBAGPT`: The fine-tuned GPT2-Chinese with the released training set.
- `model/adaptive1`: The adaptively fine-tuned GPT2-Chinese with the internal adaptive training set `adaptive1.json`.
- `model/adaptive2`: The adaptively fine-tuned GPT2-Chinese with the internal adaptive training set `adaptive2.json`.

We provide some demo data of `adaptive1.json` and `adaptive2.json` in `data\adaptive`.  You may need to further fine-tune such two models on  the released training set. Note  that, in such two datasets, the `tgt` is generated by our rule-based method.


We give a simple demo to use such checkpoints to inference:

- `demo.py`: which shows how to use the model. 

To fine-tune, you can refer to [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese) or other huggingface-based GPT2s.



# Data Processing
We provide three scripts for processing data for three checkpoints:
- `src/process_MOBAGPT.py`
- `src/process_adaptive1.py`
-  `src/process_adaptive2.py`

  


# Citation

If you like this work or need to use our dataset, please use the following citation:

```
@inproceedings{DBLP:conf/emnlp/0003W0C22,
  author    = {Dawei Zhang and
               Sixing Wu and
               Yao Guo and
               Xiangqun Chen},
  editor    = {Yoav Goldberg and
               Zornitsa Kozareva and
               Yue Zhang},
  title     = {{MOBA-E2C:} Generating {MOBA} Game Commentaries via Capturing Highlight
               Events from the Meta-Data},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
               2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022},
  pages     = {4545--4556},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.findings-emnlp.333},
  timestamp = {Tue, 07 Feb 2023 17:10:52 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/0003W0C22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

```
